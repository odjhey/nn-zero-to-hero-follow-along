{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5118beb3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "40000"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "whole_text = open('../data/tinyshakespeare.txt', 'r').read()\n",
    "lines = whole_text.splitlines()\n",
    "len(lines)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "58a33b2d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(65, \"\\n !$&',-.3:;?ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz\")"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab = sorted(set(''.join(whole_text)))\n",
    "vocab_size = len(vocab)\n",
    "vocab_size, ''.join(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ea85b1fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenizer\n",
    "stoi = {c:i for i,c in enumerate(vocab)}\n",
    "itos = {v:k for k,v in stoi.items()}\n",
    "encode = lambda str: [stoi[c] for c in str]\n",
    "decode = lambda ints: ''.join([itos[i] for i in ints])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "cd16f3e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6d516e5b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1115394]), torch.int64)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = torch.tensor(encode(whole_text))\n",
    "data.shape, data.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1dd88c85",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1003854]), torch.Size([111540]))"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n = int(data.shape[0] * .9)\n",
    "train_data = data[:n]\n",
    "val_data = data[n:]\n",
    "train_data.shape, val_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "79b7021b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([18, 47, 56, 57, 58,  1, 15, 47, 58])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "block_size = 8\n",
    "train_data[:block_size+1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "72596891",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([18]) -> 47\n",
      "tensor([18, 47]) -> 56\n",
      "tensor([18, 47, 56]) -> 57\n",
      "tensor([18, 47, 56, 57]) -> 58\n",
      "tensor([18, 47, 56, 57, 58]) -> 1\n",
      "tensor([18, 47, 56, 57, 58,  1]) -> 15\n",
      "tensor([18, 47, 56, 57, 58,  1, 15]) -> 47\n",
      "tensor([18, 47, 56, 57, 58,  1, 15, 47]) -> 58\n"
     ]
    }
   ],
   "source": [
    "# time (T) apparently (or T as in Token?)\n",
    "x = train_data[:block_size]\n",
    "y = train_data[1:block_size+1]\n",
    "for t in range(len(x),):\n",
    "  context = x[:t+1]\n",
    "  print(f'{context} -> {y[t]}')\n",
    "\n",
    "# this is apparently called T (time) dimension? or Token? \n",
    "# i think from the BTC acronym we'll see more of later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "30dfc5e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "#device = 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "12ab60e6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[63, 53, 59,  ...,  1, 47, 58],\n",
       "         [53, 59, 56,  ..., 59, 56, 43],\n",
       "         [43, 52, 58,  ...,  5, 42,  1],\n",
       "         ...,\n",
       "         [43, 60, 43,  ...,  1, 39, 58],\n",
       "         [43,  1, 58,  ..., 53, 52,  1],\n",
       "         [52,  1, 39,  ..., 42,  5, 57]], device='cuda:0'),\n",
       " torch.Size([32, 128]),\n",
       " tensor([[53, 59, 56,  ..., 47, 58,  1],\n",
       "         [59, 56,  1,  ..., 56, 43, 42],\n",
       "         [52, 58,  1,  ..., 42,  1, 46],\n",
       "         ...,\n",
       "         [60, 43, 56,  ..., 39, 58, 58],\n",
       "         [ 1, 58, 46,  ..., 52,  1, 58],\n",
       "         [ 1, 39, 40,  ...,  5, 57,  1]], device='cuda:0'),\n",
       " torch.Size([32, 128]))"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#torch.manual_seed(1337)\n",
    "# batch (B)\n",
    "block_size = 128\n",
    "batch_size = 32\n",
    "\n",
    "def get_batch(split):\n",
    "  data = train_data if split == 'train' else val_data\n",
    "  # get offset starts for all batches\n",
    "  ix = torch.randint(0, data.shape[0]-block_size, (batch_size,))\n",
    "  x = torch.stack([data[i:i+block_size] for i in ix])\n",
    "  y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
    "  x, y = x.to(device), y.to(device)\n",
    "  return x,y\n",
    "\n",
    "xb,yb = get_batch('train')\n",
    "xb,xb.shape, yb, yb.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "230aab9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 128, 65])\n",
      "torch.Size([])\n",
      "\n",
      "mXnxM,:aOJ\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "#torch.manual_seed(1337)\n",
    "\n",
    "n_embd = 384\n",
    "dropout = 0.2\n",
    "\n",
    "class Head(nn.Module):\n",
    "  def __init__(self, head_size):\n",
    "    super().__init__()\n",
    "    self.key = nn.Linear(n_embd, head_size, bias=False)\n",
    "    self.query = nn.Linear(n_embd, head_size, bias=False)\n",
    "    self.value = nn.Linear(n_embd, head_size, bias=False)\n",
    "    self.register_buffer(\"tril\", torch.tril(torch.ones(block_size, block_size)))\n",
    "    self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "  def forward(self, x):\n",
    "    B,T,C = x.shape\n",
    "    k = self.key(x)\n",
    "    q = self.query(x) # (B,T,C)\n",
    "    # note: not sure which this square root came from hmmm, we may need to recheck the paper\n",
    "    wei = k @ q.transpose(-2,-1) * C**-0.5 \n",
    "    # decoder\n",
    "    # note: review this indexing again [:T,:T]\n",
    "    wei = wei.masked_fill(self.tril[:T,:T]==0, float('-inf'))\n",
    "    wei = F.softmax(wei, dim=-1)\n",
    "    wei = self.dropout(wei)\n",
    "\n",
    "    v = self.value(x) \n",
    "    out = wei @ v\n",
    "    return out\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "  def __init__(self, num_heads, head_size):\n",
    "    super().__init__()\n",
    "    self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n",
    "    self.proj = nn.Linear(n_embd, n_embd)\n",
    "    self.dropout = nn.Dropout(dropout)\n",
    "  \n",
    "  def forward(self, x):\n",
    "    out =  torch.cat([h(x) for h in self.heads], dim=-1)\n",
    "    out = self.dropout(self.proj(out))\n",
    "    return out\n",
    "\n",
    "class FeedForward(nn.Module):\n",
    "  def __init__(self, n_embd):\n",
    "    super().__init__()\n",
    "    self.net = nn.Sequential(\n",
    "      nn.Linear(n_embd, 4 * n_embd),\n",
    "      nn.ReLU(),\n",
    "      # projection\n",
    "      nn.Linear(4 * n_embd, n_embd),\n",
    "      nn.Dropout(dropout)\n",
    "    )\n",
    "\n",
    "  def forward(self,x):\n",
    "    return self.net(x)\n",
    "\n",
    "class Block(nn.Module):\n",
    "  def __init__(self, n_embd, n_head):\n",
    "    super().__init__()\n",
    "    head_size = n_embd // n_head\n",
    "    self.sa = MultiHeadAttention(n_head, head_size)\n",
    "    self.ffwd = FeedForward(n_embd)\n",
    "    self.ln1 = nn.LayerNorm(n_embd)\n",
    "    self.ln2 = nn.LayerNorm(n_embd)\n",
    "\n",
    "  def forward(self,x):\n",
    "    x = x + self.sa(self.ln1(x))\n",
    "    x = x + self.ffwd(self.ln2(x))\n",
    "    return x\n",
    "\n",
    "class Bigram(nn.Module):\n",
    "  def __init__(self,):\n",
    "    super().__init__()\n",
    "    self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n",
    "    self.position_embedding_table = nn.Embedding(block_size, n_embd)\n",
    "\n",
    "    self.blocks = nn.Sequential(\n",
    "      Block(n_embd, n_head=4),\n",
    "      Block(n_embd, n_head=4),\n",
    "      Block(n_embd, n_head=4),\n",
    "    )\n",
    "\n",
    "    self.ln_f = nn.LayerNorm(n_embd)\n",
    "\n",
    "    self.lm_head = nn.Linear(n_embd, vocab_size)\n",
    "  \n",
    "  def forward(self, idx, targets=None):\n",
    "    B,T = idx.shape\n",
    "    # idx = (B,T) \n",
    "    tok_emb = self.token_embedding_table(idx) # -> (B, T, n_embd)\n",
    "    pos_emb = self.position_embedding_table(torch.arange(T, device=device))\n",
    "    x = tok_emb + pos_emb\n",
    "    x = self.blocks(x)\n",
    "    x =self.ln_f(x)\n",
    "    logits = self.lm_head(x) # -> (B, T, vocab_size)\n",
    "\n",
    "    if targets is None:\n",
    "      return logits, None\n",
    "\n",
    "    B,T,C = logits.shape\n",
    "    targets = targets.view(B*T)\n",
    "    loss = F.cross_entropy(logits.view(B*T,C), targets)\n",
    "    return logits, loss\n",
    "  \n",
    "  def generate(self, idx, max_tokens=10):\n",
    "    for _ in range(max_tokens):\n",
    "      idx_inrange = idx[:,-block_size:]\n",
    "      logits, loss = self(idx_inrange)\n",
    "      # take the last T as this contains the predictions for next char\n",
    "      logits = logits[:, -1, :] # (B,T,C) -> (B,C)\n",
    "      probs = F.softmax(logits, dim=-1)\n",
    "      hit = torch.multinomial(probs, num_samples=1) # note: this fn returns indices\n",
    "      idx = torch.cat((idx, hit), dim=1)\n",
    "    return idx\n",
    "\n",
    "\n",
    "m = Bigram()\n",
    "m = m.to(device)\n",
    "logits, loss = m(xb, yb)\n",
    "print(logits.shape)\n",
    "print(loss.shape)\n",
    "\n",
    "infe = m.generate(torch.zeros((1,1), dtype=torch.long, device=device), max_tokens=10)[0]\n",
    "print(decode(infe.tolist()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "2e9ca94a",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.AdamW(m.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "cb5800b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.2606, device='cuda:0', grad_fn=<NllLossBackward0>)\n"
     ]
    }
   ],
   "source": [
    "for _ in range(10_000):\n",
    "  xb,yb = get_batch(\"train\")\n",
    "  logits,loss = m(xb,yb)\n",
    "  optimizer.zero_grad(set_to_none=True)\n",
    "  loss.backward()\n",
    "  optimizer.step()\n",
    "\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "df2ecf38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.2606072425842285\n"
     ]
    }
   ],
   "source": [
    "print(loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "36f0351b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "For whitely Tybalt, as you swore a king,\n",
      "Is say: thereof your grace, thy widow toWeleve,\n",
      "is you and both, legs are that I subdue and speared\n",
      "Could be cowe a turn; then: for you to die\n",
      "Where was more in his eyes, yours, finishes, he\n",
      "should but Vienna, and kneel thee no blood of esteeming irlt\n",
      "Tullus Attony; Give to your enemies' elieves?\n",
      "\n",
      "HASTINGS:\n",
      "So Chillips we please you like on the armour.\n",
      "\n",
      "TYBALT:\n",
      "Bolded the house; I have yet stout it.\n",
      "\n",
      "MONTAGUE:\n",
      "As burthen me as come from me impossible:\n",
      "And for mortal,\n",
      "The adeep-breathe anuuted, that you makes-king\n",
      "Have ceemed in him. Lord remember pyrthy crow\n",
      "Derives no leisurp words on their stands,\n",
      "But laught me miserable at with us\n",
      "and some unatterites: he when our committed\n",
      "For every malitive a\n",
      "Tired approbribation of what he saw said thousand deface?\n",
      "To all the poor corow friar! it is to such poor.\n",
      "\n",
      "DUKE VINCENTIO:\n",
      "No, but I say, nor Paulina, or thy crown behind; but which that thou spakest to vail them in\n",
      "delight. I have it been too lately need moveables\n",
      "my spit \n"
     ]
    }
   ],
   "source": [
    "infe = m.generate(torch.zeros((1,1), dtype=torch.long, device=device), max_tokens=1023)[0]\n",
    "print(decode(infe.tolist()))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mymakemore (3.13.11)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
