{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 199,
   "id": "5118beb3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "40000"
      ]
     },
     "execution_count": 199,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "whole_text = open('../data/tinyshakespeare.txt', 'r').read()\n",
    "lines = whole_text.splitlines()\n",
    "len(lines)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "id": "58a33b2d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(65, \"\\n !$&',-.3:;?ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz\")"
      ]
     },
     "execution_count": 200,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab = sorted(set(''.join(whole_text)))\n",
    "vocab_size = len(vocab)\n",
    "vocab_size, ''.join(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "id": "ea85b1fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenizer\n",
    "stoi = {c:i for i,c in enumerate(vocab)}\n",
    "itos = {v:k for k,v in stoi.items()}\n",
    "encode = lambda str: [stoi[c] for c in str]\n",
    "decode = lambda ints: ''.join([itos[i] for i in ints])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "id": "cd16f3e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "id": "6d516e5b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1115394]), torch.int64)"
      ]
     },
     "execution_count": 203,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = torch.tensor(encode(whole_text))\n",
    "data.shape, data.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "id": "1dd88c85",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1003854]), torch.Size([111540]))"
      ]
     },
     "execution_count": 204,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n = int(data.shape[0] * .9)\n",
    "train_data = data[:n]\n",
    "val_data = data[n:]\n",
    "train_data.shape, val_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "id": "79b7021b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([18, 47, 56, 57, 58,  1, 15, 47, 58])"
      ]
     },
     "execution_count": 205,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "block_size = 8\n",
    "train_data[:block_size+1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "id": "72596891",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([18]) -> 47\n",
      "tensor([18, 47]) -> 56\n",
      "tensor([18, 47, 56]) -> 57\n",
      "tensor([18, 47, 56, 57]) -> 58\n",
      "tensor([18, 47, 56, 57, 58]) -> 1\n",
      "tensor([18, 47, 56, 57, 58,  1]) -> 15\n",
      "tensor([18, 47, 56, 57, 58,  1, 15]) -> 47\n",
      "tensor([18, 47, 56, 57, 58,  1, 15, 47]) -> 58\n"
     ]
    }
   ],
   "source": [
    "# time (T) apparently (or T as in Token?)\n",
    "x = train_data[:block_size]\n",
    "y = train_data[1:block_size+1]\n",
    "for t in range(len(x),):\n",
    "  context = x[:t+1]\n",
    "  print(f'{context} -> {y[t]}')\n",
    "\n",
    "# this is apparently called T (time) dimension? or Token? \n",
    "# i think from the BTC acronym we'll see more of later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "id": "12ab60e6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[24, 43, 58,  5, 57,  1, 46, 43],\n",
       "         [44, 53, 56,  1, 58, 46, 39, 58],\n",
       "         [52, 58,  1, 58, 46, 39, 58,  1],\n",
       "         [25, 17, 27, 10,  0, 21,  1, 54]]),\n",
       " torch.Size([4, 8]),\n",
       " tensor([[43, 58,  5, 57,  1, 46, 43, 39],\n",
       "         [53, 56,  1, 58, 46, 39, 58,  1],\n",
       "         [58,  1, 58, 46, 39, 58,  1, 46],\n",
       "         [17, 27, 10,  0, 21,  1, 54, 39]]),\n",
       " torch.Size([4, 8]))"
      ]
     },
     "execution_count": 207,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(1337)\n",
    "# batch (B)\n",
    "block_size = 8\n",
    "batch_size = 4\n",
    "\n",
    "def get_batch(split):\n",
    "  data = train_data if split == 'train' else val_data\n",
    "  # get offset starts for all batches\n",
    "  ix = torch.randint(0, data.shape[0]-block_size, (batch_size,))\n",
    "  x = torch.stack([data[i:i+block_size] for i in ix])\n",
    "  y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
    "  return x,y\n",
    "\n",
    "xb,yb = get_batch('train')\n",
    "xb,xb.shape, yb, yb.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "id": "230aab9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 8, 65])\n",
      "torch.Size([])\n",
      "\n",
      "?\n",
      "fq:zj?dB\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "torch.manual_seed(1337)\n",
    "\n",
    "n_embd = 32\n",
    "\n",
    "class Head(nn.Module):\n",
    "  def __init__(self, head_size):\n",
    "    super().__init__()\n",
    "    self.key = nn.Linear(n_embd, head_size, bias=False)\n",
    "    self.query = nn.Linear(n_embd, head_size, bias=False)\n",
    "    self.value = nn.Linear(n_embd, head_size, bias=False)\n",
    "    self.register_buffer(\"tril\", torch.tril(torch.ones(block_size, block_size)))\n",
    "\n",
    "  def forward(self, x):\n",
    "    B,T,C = x.shape\n",
    "    k = self.key(x)\n",
    "    q = self.query(x) # (B,T,C)\n",
    "    # note: not sure which this square root came from hmmm, we may need to recheck the paper\n",
    "    wei = k @ q.transpose(-2,-1) * C**-0.5 \n",
    "    # decoder\n",
    "    # note: review this indexing again [:T,:T]\n",
    "    wei = wei.masked_fill(self.tril[:T,:T]==0, float('-inf'))\n",
    "    wei = F.softmax(wei, dim=-1)\n",
    "\n",
    "    v = self.value(x) \n",
    "    out = wei @ v\n",
    "    return out\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "  def __init__(self, num_heads, head_size):\n",
    "    super().__init__()\n",
    "    self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n",
    "    self.proj = nn.Linear(n_embd, n_embd)\n",
    "  \n",
    "  def forward(self, x):\n",
    "    out =  torch.cat([h(x) for h in self.heads], dim=-1)\n",
    "    out = self.proj(out)\n",
    "    return out\n",
    "\n",
    "class FeedForward(nn.Module):\n",
    "  def __init__(self, n_embd):\n",
    "    super().__init__()\n",
    "    self.net = nn.Sequential(\n",
    "      nn.Linear(n_embd, 4 * n_embd),\n",
    "      nn.ReLU(),\n",
    "      # projection\n",
    "      nn.Linear(4 * n_embd, n_embd),\n",
    "    )\n",
    "\n",
    "  def forward(self,x):\n",
    "    return self.net(x)\n",
    "\n",
    "class Block(nn.Module):\n",
    "  def __init__(self, n_embd, n_head):\n",
    "    super().__init__()\n",
    "    head_size = n_embd // n_head\n",
    "    self.sa = MultiHeadAttention(n_head, head_size)\n",
    "    self.ffwd = FeedForward(n_embd)\n",
    "\n",
    "  def forward(self,x):\n",
    "    x = x + self.sa(x)\n",
    "    x = x + self.ffwd(x)\n",
    "    return x\n",
    "\n",
    "class Bigram(nn.Module):\n",
    "  def __init__(self,):\n",
    "    super().__init__()\n",
    "    self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n",
    "    self.position_embedding_table = nn.Embedding(block_size, n_embd)\n",
    "\n",
    "    self.blocks = nn.Sequential(\n",
    "      Block(n_embd, n_head=4),\n",
    "      Block(n_embd, n_head=4),\n",
    "      Block(n_embd, n_head=4),\n",
    "    )\n",
    "\n",
    "    self.lm_head = nn.Linear(n_embd, vocab_size)\n",
    "  \n",
    "  def forward(self, idx, targets=None):\n",
    "    B,T = idx.shape\n",
    "    # idx = (B,T) \n",
    "    tok_emb = self.token_embedding_table(idx) # -> (B, T, n_embd)\n",
    "    pos_emb = self.position_embedding_table(torch.arange(T))\n",
    "    x = tok_emb + pos_emb\n",
    "    x = self.blocks(x)\n",
    "    logits = self.lm_head(x) # -> (B, T, vocab_size)\n",
    "\n",
    "    if targets is None:\n",
    "      return logits, None\n",
    "\n",
    "    B,T,C = logits.shape\n",
    "    targets = targets.view(B*T)\n",
    "    loss = F.cross_entropy(logits.view(B*T,C), targets)\n",
    "    return logits, loss\n",
    "  \n",
    "  def generate(self, idx, max_tokens=10):\n",
    "    for _ in range(max_tokens):\n",
    "      idx_inrange = idx[:,-block_size:]\n",
    "      logits, loss = self(idx_inrange)\n",
    "      # take the last T as this contains the predictions for next char\n",
    "      logits = logits[:, -1, :] # (B,T,C) -> (B,C)\n",
    "      probs = F.softmax(logits, dim=-1)\n",
    "      hit = torch.multinomial(probs, num_samples=1) # note: this fn returns indices\n",
    "      idx = torch.cat((idx, hit), dim=1)\n",
    "    return idx\n",
    "\n",
    "\n",
    "m = Bigram()\n",
    "logits, loss = m(xb, yb)\n",
    "print(logits.shape)\n",
    "print(loss.shape)\n",
    "\n",
    "infe = m.generate(torch.zeros((1,1), dtype=torch.long), max_tokens=10)[0]\n",
    "print(decode(infe.tolist()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "id": "2e9ca94a",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.AdamW(m.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "id": "cb5800b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.8891, grad_fn=<NllLossBackward0>)\n"
     ]
    }
   ],
   "source": [
    "batch_size=32\n",
    "for _ in range(10_000):\n",
    "  xb,yb = get_batch(\"train\")\n",
    "  logits,loss = m(xb,yb)\n",
    "  optimizer.zero_grad(set_to_none=True)\n",
    "  loss.backward()\n",
    "  optimizer.step()\n",
    "\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "id": "df2ecf38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.8890974521636963\n"
     ]
    }
   ],
   "source": [
    "print(loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "id": "36f0351b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "CANTIONE:\n",
      "Nay unlic;\n",
      "And peavend,\n",
      "Is there of may: but like eye.\n",
      "O make be head,\n",
      "And lawn, be!\n",
      "Why, a bloid 'I?\n",
      "I shood be forth\n",
      "Coideepree; as serving of for will it as my somen thou head\n",
      "To Client: bone\n",
      "MAWI:\n",
      "\n",
      "Edward.\n",
      "\n",
      "JULIO:\n",
      "How miert hust,\n",
      "To cick and tritalt, think meady\n",
      "Shall I noot:\n",
      "Comed frauself wome's put the LAnchitice\n",
      "And had as admose: nep, but to her bestratice more you matteat o'a celts;\n",
      "Not my heoplonourt Not no my plaid, by the doe? not at I some.\n",
      "\n",
      "PELOUCHESS:\n",
      "Westit all be a ferea, and of but ou with and of I come to bence's flauded can I what youst sight.\n",
      "\n",
      "POLIFFOLYCUS:\n",
      "I cute-\n",
      "Are as mother you of Irom!\n",
      "\n",
      "POMPIFFO:\n",
      "Why, wom grave afferather me my the some of my pruch,\n",
      "The mery so by his enting his I for, nest just you sue of dight,\n",
      "Thou any my him defults, do forthorparlas:\n",
      "Ay, my not, my son\n",
      "Rom sover officious we do you, scominers! hy kink youners, too I drope.\n",
      "\n",
      "Thy know the piermas, anto now's of Ell a prome wich pesick in a no he demans: but Anger shord in it reblent:\n",
      "Fooward commonter\n"
     ]
    }
   ],
   "source": [
    "infe = m.generate(torch.zeros((1,1), dtype=torch.long), max_tokens=1023)[0]\n",
    "print(decode(infe.tolist()))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mymakemore (3.13.11)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
